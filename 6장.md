# 6장

## <06-01 신경망 알고리즘의 벡터화>

### 배치 경사 하강법

배치는 전체 샘플을 가지고 경사하강법 한 스텝을 처리하는 것

이전까지는 훈련세트에서 샘플 하나를 뽑아 그것으로 정방향, 역방향 계산해 그래디언트를 계산하고 가중치를 업데이트 했음

이 과정은 최저점을 찾아가는 경로가 빠르긴 하지만 들쭉날쭉함

반대로 배치를 사용하면 전체 샘플에 대해 정방향, 역방향 계산을 하고 가중치를 업데이트 하므로 최저점을 찾아가는 방향은 올바른 방향으로 찾아가지만 수행속도는 느리다

확률적 경사 하강법과 배치 경사 하강법의 절충안이 미니 배치 경사 하강법임
<img src ="https://github.com/yoon-303/Study/blob/main/6_imgs/Untitled%201.png">
![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled.png)

점곱 : 벡터끼리의 곱셈, 행렬 곱셈

샘플이 여러개일 경우를 위해 np.dot 사용

(m, n)*(n, k) = (m, k)

### cancer 데이터셋의 정방향 계산

x에 364개 샘플, 30개 특성이 있었음. ⇒ 가중치 w도 30개 가지고있음 이를 행렬 곱셈에 적용 위해  30개 행, 1개 열인 행렬로 만듦

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%201.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%201.png)

np.sum → np.dot 으로 바꿔서 전체 샘플에 대해 행렬 곱셈 가능

### cancer 데이터셋의 역방향 계산

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%202.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%202.png)

샘플마다 에러가 발생하니 에러는 364개 만들어지게 됨

이것을 열벡터로 놓고, 훈련데이터 x를 전치함 (행과 열을 바꾸는 것) ⇒ (30, 364) 행렬이 된다

각 특성이 행, 샘플이 열 이 됨 각 샘플과 에러를 행렬곱셈하기 위해 전치시킨것

샘플 전체에 대해 그래디언트를 계산하였기 때문에 샘플 개수인 m = len(x)로 나눠서 평균값을 계산해야 함

### fit() 메서드 수정

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%203.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%203.png)

경사 하강법에서는 에포크마다 permu로 인덱스를 섞어서 섞은 인덱스 순으로 샘플을 하나씩 꺼내서 정방향계산, 활성화함수적용, 오차계산을 진행함.

하지만 전체 샘플을 사용하니 for문은 사라지게 됨

## <06-02 2개의 층을 가진 신경망 구현>

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%204.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%204.png)

x1, x2, x3 를 두 뉴런마다 다른 가중치를 곱함

### 출력을 하나로 모읍니다

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%205.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%205.png)

활성화 함수까지를 보통 은닉층이라고 말하고 그 이후를 출력층이라 함

샘플이 1개 이상이라면 활성화 출력도 1개 이상이 출력됨

은닉층의 뉴런이 여러개라면 가중치의 행렬도 늘어남

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%206.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%206.png)

입력층은 준비된 데이터, 실제 계산이 일어나는 층은 은닉층, 출력층 이전의 모든 층은 은닉층이 됨 출력층에서는 최종 아웃풋 z가 계산됨

### 다층 신경망 개념 정리

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%207.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%207.png)

다층 신경망은 입력층이 있고, 은닉층에 이 모든 데이터가 전달됨. z1~zm은 출력층의 모든 뉴런에 전달됨

각 층의 데이터와 뉴런이 모두 연결돼있음

활성화함수는 모든 은닉층, 출력층에 존재함(회귀의 경우엔 출력층에는 활성화 없음). 한 층의 활성화 함수는 같은 것을 사용해야 함

### 다층 신경망에 경사 하강법 적용

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%208.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%208.png)

뉴런이 여러개 있을 수 있음 가중치를 행렬로 표현한 것

### 가중치에 대해 손실 함수 미분(출력층)

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%209.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%209.png)

시그모이드 함수를 미분하면 -(Y-A)

w에 대해 z 미분하면 A만 남음

A는 은닉층에서 온 전달값

출력층의 뉴런 입장에서 이것은 입력이 됨

A는 샘플마다 행으로 놓여 있음 (m개 샘플 시 m개 행)

(m, 2)행렬을 (m,1) 행렬인Y-A2와 곱하기 위해 (2,m)행렬로 전치를 해서 바꾼다

### 절편에 대해 손실 함수 미분(출력층)

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2010.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2010.png)

### 가중치에 대해 손실 함수 미분(은닉층)

손실함수 미분을 보면 Z2미분, A1미분, 마지막으로 Z1을 나눠서 미분함

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2011.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2011.png)

### 도함수를 곱합니다(은닉층) - 1

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2012.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2012.png)

### 도함수를 곱합니다(은닉층) - 2

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2013.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2013.png)

(m,3) X를 행렬곱셈 위해 전치해서 (3,m)행렬로 바꿔서 (m,2)와 곱함 →(3,2)행렬이 나옴 w1과 사이즈 동일함.

행렬곱셈을 하기위해 차원을 맞춰가며 계산한 것

### 절편에 대해 손실함수 미분, 도함수 곱함

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2014.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2014.png)

(1,2) 행렬이 나오게 됨

### 2개의 층 가진 신경망 구현하기

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2015.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2015.png)

np.dot 이용해 선형식 계산 후 활성화 함수 적용해줌

그 후 다음 층 선형식 계산을 해줌

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2016.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2016.png)

err은 이미 -(Y-A2)로 계산되어 있음

### int_weights() 메서드 추가하고 훈련

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2017.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2017.png)

가중치 초기화가 중요하기 때문에 메서드를 추가함

절편은 0, w는 1로 초기화

가중치는 뉴런의 개수가 열, 특성개수가 행

n_features가 행, 유닛(뉴런)개수가 열 차원이 됨

### 가중치 초기화 개선하기

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2018.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2018.png)

시작 지점이 좋아야 최저점을 찾아가는데 좋음

랜덤 함수 사용해서 평균 0, 표준편차 1인 정규분포로 가중치의 값을 임의의 난수로 초기화함

일반적으로 절편은 0으로 초기화

## <06-03 미니 배치 경사 하강법 구현>

미니배치는 배치의 작은 버전으로 샘플의 일부분을 랜덤하게 뽑아 경사하강법 한단계 수행 후 가중치 업데이트, 또 미니배치로 랜덤하게 뽑아 진행 후 가중치 업데이트를 함

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2019.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2019.png)

미니배치경사하강법은 배치의 크기를 지정해줘야 함(하이퍼파라미터)

여러번의 시행을 통해 결정, 일반적으론 계산의 효율성을 위해2의 배수 사용함

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2020.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2020.png)

x,y는 전체 데이터셋이고 그 중 일부를 엑스와이배치로 반환해줌

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2021.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2021.png)

훈련 시 그래디언트를 미니 배치의 개수 만큼 나눠줌

파이썬의 제너레이터 함수는 return이 아닌 yield로 반환

### 미니 배치 경사 하강법 훈련

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2022.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2022.png)

미니 배치사이즈 32일때 훈련속도가 굉장히 빠르고 

미니 배치사이즈 128일때 훈련속도는 조금 더 느림

보통 사이즈는 32~512 정도에서 2의 배수를 선택해서 사용한다

### 사이킷런 사용해 다층 신경망 훈련하기

![6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2023.png](6%E1%84%8C%E1%85%A1%E1%86%BC%2097a80d89353a4bb5869536b4ea918fbf/Untitled%2023.png)

MLPClassifier 사용

회귀에 사용하는 신경망

히든레이어사이즈를 미리 정해줘야 함 활성화함수 로지스틱 사용 (모든 층에 동일한 활성화 함수 사용)
