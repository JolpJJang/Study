# 딥러닝06장정리

이 전까지는 단일층 신경망을 배웠음. 6장부터는 다층 신경망!

**06-1. 신경망 알고리즘의 벡터화**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image1.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image1.png)

배치 경사 하강법은 전체 sample로 정방향계산 ▶ 역방향계산도 전체 sample로 진행 ▶ 결과로가중치 업데이트

장점 : 가장 올바른 방향으로 최적점을 찾아간다.

단점 : 수행속도가 느리다, 컴퓨터 자원 활용이 크다.

- *원소끼리 곱해지는 계산 = 점곱 ( [x1 x2 x3] * [w1 w2 w3] )

> **점곱이 벡터가 아닌 행렬사이의 곱셈이 되면 행렬곱셈이라 한다.(명확하게 나누지는 않음)**np.sum 을 np.dot으로 바꾼 이유는 행렬을 계산할 수 있기 때문이다. 자동으로 최적화된 계산 시행!

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image2.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image2.png)

- 참고* 벡터는 굵은 소문자, 행렬은 굵은 대문자로 쓴다.

**<전체 sample에 대해 행렬 곱셈 적용하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image3.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image3.png)

> **특성은 열에 하나씩 위치해 있음**(m,3) * (3,1) ▶ (m,1)행렬

**<cancer 데이터셋의 정방향 계산>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image4.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image4.png)

- *X가 364개의 sample, 30개의 특성! 가중치도 30.
- *b는 다 똑같은 값을 가짐(하나의 값이기 때문)

    ![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image5.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image5.png)

- *따라서 굳이 364개를 다 복사할 필요X! 끝에 +b를 해줌

    ![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image6.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image6.png)

왼쪽 식으로 바꿈(np.sum → np.dot)으로 인해 하나씩 꺼내서 쓰는게 아닌 전체 sample을 다 넣을 수 있게 됨!

**<cancer 데이터셋의 역방향 계산>**

x*err

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image7.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image7.png)

**훈련데이터X를 전치시킴 (바꾼이유는 364개의 error를 364개의 sample과 곱해주기 위해서임)

- *각 특성이 행에, sample이 열에 놓이게 됨
- *error가 x특성 각각에 적용이 되어 그레이디언트 1,2,3… ▶ 가중치 w1,w2,w3에 update됨

    ![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image8.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image8.png)

- *위와 마찬가지로 np.dot()함수 이용 ▶ x의 전치행렬과 err곱해서 sample개수(m)로 나눠줌
- *절편(b)의 경우는 np.sum(err)을 써도 상관X

**<fit() method 수정>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image9.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image9.png)

★경사하강법 에서는 에포크마다 numpy의 함수(permutation)를 사용 → 인덱스를 섞어서 하나씩 샘플을 꺼냄 → 정방향 계산하고 활성화함수 적용 → 오차함수계산 **But** 전체sample 사용하니까 에포크만큼 반복하고 그냥 전체x를 집어넣고 활성화값 계산 → 에러계산 후 역방향 계산 진행

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image10.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image10.png)

**★전체 sample을 사용하면 손실이 부드럽게 진행된다.**

**★w[2]와 w[3]도 일정한 경로를 따라 부드럽게 변화한다.(점이 최적점에 가까운 값이 됨)**

**06-2. 2개의 층을 가진 신경망 구현하기**

**<여러 개의 뉴런 사용하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image11.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image11.png)

- *z1, z2 두개의 뉴런 // x1,x2,x3 세개의 입력층
- *입력이 두 개의 뉴런에 각각 연결됨(완전연결층, 밀집층)
- *곱해지는 가중치는 뉴런마다 다름

**<출력을 하나로 모으기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image12.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image12.png)

- *첫번째 은닉층의 결과가 활성화함수를 거쳐서 출력층에 도달
- *은닉층 : 활성화함수를 포함하는 구간까지 // 출력층 : 그 이후 구간
- *마지막 출력층 뉴런은 하나 이상일 수 있음
- *sample이 한 개 이상이라면 출력도 한 개 이상! 두번째 활성화출력은 두번째 층에, 세번째 출력은 세번째 층에 놓이게 됨

**<은닉층이 추가된 신경망>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image13.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image13.png)

**<다층 신경망 개념 정리>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image14.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image14.png)

- *은닉층에 x가 모두 전달, z가 출력층에 모두 전달!(완전 연결 신경망, 밀집 신경망)
- *만약 회귀문제라면 출력층에 활성화함수가 없음. 이 경우 제외하고는 출력층에 모두 활성화함수가 존재해야 함
- *은닉층의 활성화 함수와 출력층의 활성화함수는 다른 함수 적용
- *데이터가 다 앞으로 전달됨 ▶ 피드 포워드 신경망(합성곱 신경망도 피드 포워드 신경망임)

**<다층 신경망에 경사 하강법 적용하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image15.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image15.png)

- *뉴런이 그림상에는 한 개이지만 실제로는 여러 개임**

**<가중치에 대해 손실함수 미분하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image16.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image16.png)

- *A1는 sample마다 행으로 놓여있음
- *원래 (m,2)인 행렬A1를 전치시켜 (2,m)으로 바꾼 후, (m,1)인 Y-A2와 곱셈!

**<절편에 대해 손실함수 미분하기(출력층)>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image17.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image17.png)

- *전체 sample에 대한 계산이므로 그레이디언트의 합임

**<가중치에 대해 손실함수 미분하기(은닉층)>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image18.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image18.png)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image19.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image19.png)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image20.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image20.png)

**<절편에 대해 손실함수 미분하고 도함수 곱하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image21.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image21.png)

**<코드로 구현>**

1. 정방향 계산

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image22.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image22.png)

- *a1을 입력으로 넣어서 np.dot 계산
1. 역방향 계산

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image23.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image23.png)

- * -(Y-A2) = err
- *axis=0으로 놓으면 1*2행렬, axis=1로 놓으면 m*1행렬

**<init_weights() method 추가하고 훈련하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image24.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image24.png)

- *init_weights() method를 따로 만든 이유 : 가중치 초기화가 중요하고, 한 개의 method를 너무 길게 하지 않기 위해서
- *(특성개수,뉴런개수) = (n_features, self.units)
- *b2는 값이 하나니까 단순하게 0으로 초기화
- *굴곡과 변동이 조금 있는 상태

**<가중치 초기화 개선하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image25.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image25.png)

- *층이 많아지고 뉴런 개수가 많아지면 최적점을 찾아가는 데에 출발점이 중요해짐!!!
- *랜덤함수를 사용하여 평균이0, 표준편차가1인 정규분포로 가중치의 값을 임의의 난수를 뽑아 초기화
- *아주 부드러운 곡선(훈련이 잘 됨을 알 수 있음)
- *가중치 초기화가 큰 역할을 한다는 것을 알 수 있음(출발점 중요)

**06-3. 미니 배치 경사 하강법**

확률적 경사 하강법 : 가중치의 최적값을 찾는데 들쭉날쭉 이동

배치 경사 하강법 : 굉장히 부드럽게 손실함수가 감소(전체 훈련세트를 이용했기 때문. But 훈련에 많은 시간과 비용이 발생)

★따라서 이 둘의 중간지점인 미니배치 경사 하강법을 사용★

미니 배치 경사 하강법 : sample의 일부분을 랜덤하게 뽑아서 경사 하강법 한단계를 수행, 가중치 업데이트, 다시 랜덤하게 뽑아서 경사 하강법 단계 진행, 가중치 업데이트…

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image26.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image26.png)

- *수정해야 할 점 : batch_size 지정(하이퍼파라미터). 일반적으로 2의배수를 권장함(계산의 효율성을 위해서)
- *바깥쪽 for루프에서는 에포트 횟수만큼 반복, 안쪽 for문에서는 일부만 뽑아서 x배치와y배치로 반환…데이터없을때까지 반복. y배치를 열백터로 바꿈. 미니배치의 개수=m

**<미니 배치 경사 하강법 훈련>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image27.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image27.png)

- *미니 배치 하강법에서 배치 사이즈가 32일 때 훈련속도 굉장히 빠름
- *배치 경사 하강법은 훈련속도가 느린 것을 볼 수 있음
- *배치 사이즈128의 미니 배치 하강법은 미니배치(사이즈32)와 배치의 중간속도
- *배치 사이즈는 손실함수와 정확도를 보고 찾아야 함! 대부분 32~512사이의 2의배수를 선택

**<sklearn을 사용하여 다층 신경망 훈련하기>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image28.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC06%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%20355f84b26ccb4af2b6128d054af733d4/image28.png)

- *입력데이터를 넣으면 입력층은 자동으로 파기되기 때문에 hidden_layer_size만 정의해주면 됨
- *활성화함수=logistic
- *사이킷런의 MLPClassifier는 모든 층의 활성화함수가 동일해야 함 ▶ 간단한 테스트에 사용
- *확률적 경사 하강법 사용(slover=’sgd’)
- *반복횟수 에포크 최대 500(max_iter=500)