# 딥러닝05장정리

**05-1. 검증 세트 나누고 전처리 과정**

테스트 set로 모델 성능 확인

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image1.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image1.png)

x_train_all과 y_train_all을 사용하여 맨 첨에 loss함수를 log로 놓고 로지스틱 회귀 모델을 훈련했었음 → 성능이 약 83%였음

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image2.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image2.png)

이것이 부족하여 loss = hinge로 바꾸었음(서포트 벡터 머신(SVM)) → 경사하강법분류기로 분류 → 성능 약 94%가 나옴

모델 파라미터 → 가중치

사람이 정해줘야하는 것들을 “하이퍼파라미터” 라고 부름(클래스, 매서드 매개변수) → 데이터에서 학습할 수 없기 때문에 여러 번 실험을 거듭해서 찾아야 함!!

좋은 테스트를 위해 매개변수를 바꾸다보면 Test set에 훈련하는 것 같은 효과를 얻게 됨

가능하면 Test set을 사용하지않고 1~2번만 사용 → 그때 얻은 점수가 실전에 투입했을 때의 성능이라고 평가할 수 있음

훈련세트로만은 모델을 튜닝하기 어려움 → 훈련세트를 조금 더 나눔 ex)이전에는 80%를 훈련세트로 사용, 이번에는 60%를 훈련세트로, 20%는 검증세트(개발세트)로 활용. 검증세트에서 하이퍼파라미터를 튜닝함(반복해서 여러 번 시행) → best model찾아 일반화성능 추정

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image3.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image3.png)

- 테스트세트를 가능한 건드리지 않고 좋은 점수가 나오는 모델을 얻어야 함!

@데이터 전처리와 특성의 스케일 알아보기@

스케일이 크면 가중치에 큰 영향을 미침

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image4.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image4.png)

따라서 w1은 변화량이 작고, w2는 변화량이 크다.

특성의 스케일을 바꿔줄 때

클래스에서 learning_rate추가(학습률)

w_update에 lr를 곱함 (손실함수그래프에서 전역 최소값을 찾아갈 때 가중치가 너무 빨리 변하면 그 값을 지나칠 수 있기 때문에 w를 조금씩 이동시키기 위해서 lr를 아주 작은 값으로 설정함. 교재에서는 0.1로 두고 계산)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image5.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image5.png)

w2는 0~2000, w3는 0~600까지 값이 변하는 것을 볼 수 있음

w2의 변화율은 완만한 형태를 띄는 반면, w3는 변화율이 부드럽지 못함 → 최종 가중치를 찾아가는데 낭비가 심함!(비효율적)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image6.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image6.png)

스케일을 조정하는 대표적인 방법 : 표준화(standardization)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image7.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image7.png)

np.mean으로 평균 계산, np.std로 표준편차 계산

**x_train_scaled = (x_train – train_mean) / train_std**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image8.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image8.png)

위 그림을 보면 최저점을 smooth하고 빠르게 찾아가는 것을 볼 수 있음

- ***신경망은 꼭 표준화를 해야 함****

@검증세트를 사용하여 모델 평가하기@

x_val도 전처리를 해야함(안하면 결과가 아주 나쁘게 나옴)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image9.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image9.png)

왼쪽그림은 원본데이터를 이용, 오른쪽 그림은 x_train_scaled와 x_val_scaled를 이용

train_set 과 val_set의 위치가 조금 더 멀어진 것을 볼 수 있음 → 빼는 값과 나누는 값이 달라졌기 때문!

???원래 x_train_scaled 데이터가 효과를 발휘하지 못함???

검증세트를 변환할 때 훈련데이터의 평균과 표준편차를 이용함!(검증세트의 평균과 표준편차를 이용하지 않음!)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image10.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image10.png)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image11.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image11.png)

**05-2. 과대적합과 과소적합**

- * 정확도와 훈련세트의 크기로 과대적합과 과소적합을 나눔

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image12.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image12.png)

과대적합 : 훈련세트를 과도하게 학습해서 정확도가 높고 손실이 낮지만 검증세트에서는 정확도가 낮고 손실이 낮게 나오지 않음. 훈련세트의 크기가 작으면 정확도가 매우 높음! But 검증세트의 정확도는 낮아짐. **훈련세트의 크기를 크게 해도 각각의 정확도의 차이가 큼

과소적합 : 훈련세트 크기가 커지면서 각각의 정확도가 근사하지만 낮아짐. 편향이 큼

적당한 기준을 잡아야 함!(맨 오른쪽 그림처럼)

- * 일반적으로 딥러닝 에서는 모델을 과대적합(일반적으로 뉴런이나 층의 개수를 늘리는 방법)시키고 규제(가중치를 규제, 드롭아웃과 같은 방법)를 통해 적절한 모델을 만듦

**에포크 vs 손실함수**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image13.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image13.png)

@최적점보다 많이 훈련 → 과대적합

@최적점보다 적게 훈련 → 과소적합

에포크가 진행될수록

1) 과대적합된 모델이 만들어짐

2) 훈련세트의 손실값이 줄어듦. 검증세트의 손실은 점점 줄어들다가 어느지점(최적점)부터는 상승하게 됨

손실값을 개선해도 정확도가 즉각적으로 오르지 않음

**모델 복잡도 vs 손실함수**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image14.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image14.png)

에포크 반복 → 모델의 복잡도↑

- *** 적절한 편향-분산 트레이드오프 선택하기**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image15.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image15.png)

20~40 구간에서 손실값이 높아짐을 확인 → 따라서 20정도에서 훈련을 멈춘 것임!(epochs=20)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image16.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image16.png)

모델의 정확도 출력 약 97.8%

- * 적절한 에포크횟수 찾기!

**05-3. 가중치 규제를 단일층 신경망에 적용하기**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image17.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image17.png)

어떤 모델이 더 일반화가 잘 되어있을까? 어떤것이 더 나은 정확도를 얻을 수 있을까?(특성x, 타깃값y)

파란색 모델이 더 좋음을 직관적으로 확인! ▶ 그래프의 기울기가 작을수록 일반화가 좋다고 말할 수 있음

(오른쪽 모델과 같은 경우는 굉장히 복잡해지고 일반화될 수 없음)

- * 기울기를 줄인다는 것 = 가중치를 줄인다는 것

너무 가중치가 적으면 과소적합된 모델이 만들어짐! 적절한 중간지점(트레이드오프)을 찾아야 함

**<L1 규제>**

손실함수 + L1 노름(norm) // 가중치의 절대값을 추가하는 것임

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image18.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image18.png)

간단하게 **w_grad += alpha * np.sign(w)**

**<L2 규제>**

(L1 규제는 영향력이 약함. L2 규제는 가중치가 좀 더 큰 영향을 미치기 때문에 더 많이 사용)

손실함수 + L2 노름(norm)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image19.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image19.png)

**w_grad += alpha * w**

**<L1 규제와 L2 규제 정리>**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image20.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image20.png)

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image21.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image21.png)

l1은 L1규제에서 alpha파라미터에 대응, l2는 L2규제에서 alpha파라미터에 대응

self.l1=0이면 L2규제만 적용, 반대라면 L1규제만 적용가능

선형회귀에 L1+L2 규제를 사용하는 것을 “엘라스틱 넷” 이라고 한다.

<로지스틱 회귀 + L1 규제>

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image22.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image22.png)

l1의 값이 커지면 검증세트의 손실이 오히려 높아짐! 자유도가 억제됨(0.01일 때 과소적합)

l1이 0.0001에서 0.001로 높아졌을 때 가중치가 0에 좀 더 가까워짐

<로지스틱 회귀 + L2 규제>

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image23.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image23.png)

L2규제를 더 선호!

- * 손실함수의 위치를 높이거나 낮춤. 실제 모델의 그래프를 실제로 바꾸진 않음. 따라서 가중치만 규제의 대상이 됨.

**05-4. 교차검증**

![%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image24.png](%E1%84%83%E1%85%B5%E1%86%B8%E1%84%85%E1%85%A5%E1%84%82%E1%85%B5%E1%86%BC05%E1%84%8C%E1%85%A1%E1%86%BC%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%206ba7ca2834fe43d3b468949fbf6e30c9/image24.png)

1. 테스트 세트를 떼어놓는다.
2. 나머지 조각을 5개로 나눔(fold)
3. 첫째 폴드를 검증폴드로. 나머지 네개는 훈련폴드
4. 검증폴드를 2번째로 바꿈. 나머지를 훈련폴드로.
5. 다섯번 반복
- * 딥러닝모델에서는 교차검증을 잘 사용하지 않음. 머신러닝에서 많이 사용