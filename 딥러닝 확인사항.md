# 과적합 시 고려해야될 사항

## Data Augmentation
- 데이터 량 늘림

```python
### 케라스 데이터 증강 ###
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#datagen = ImageDataGenerator()
datagen = ImageDataGenerator(rotation_range = 10, horizontal_flip = True, zoom_range = 0.1)

### 파이토치 데이터 증강 ###
from torchvision import transforms
transforms.Compose([
				transforms.CenterCrop(10),
				transforms.ToTensor(),
				transforms.Rescale(256),
				transforms.RandomCrop(224),
])
```

## Weight Regularization
- L1 norm ('Lasso')
- L2 norm ('Ridge'): 유클리드 거리

## Weight Initialization
신경망이 깊어질수록 분포가 한쪽으로 쏠릴 수 있어서 Gradient Vanishing이 나타날 수도 있음  
이를 대비하기 위한 가중치 초기화
| Xavier 초기화 | KamingHe 초기화 |
|:----------:|:----------:|
|활성화함수가 Sigmoid나 Tanh|활성화함수가 ReLU|
### Xavier 초기화
```python
### 텐서플로우 Xavier 초기화 ###
W = tf.get_variable("W", shape=[784, 256],
           initializer=tf.contrib.layers.xavier_initializer())
           
### 케라스 Xavier 초기화 ###
model = Sequential()
model.add(Dense(50, kernel_initializer='he_normal'))

### 파이토치 Xavier 초기화 ###
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)

# xavier initialization
torch.nn.init.xavier_uniform_(linear1.weight)
torch.nn.init.xavier_uniform_(linear2.weight)
torch.nn.init.xavier_uniform_(linear3.weight)
```
### He 초기화
uniform 분포를 따르는 경우랑 Normal 분포를 따르는 경우
```python
### 텐서플로우 Xavier 초기화 ###
# 01
W1 = tf.get_variable('W1', shape=[784, 256],
       initializer=tf.contrib.layers.variance_scaling_initializer())
# 02       
initializer = tf.contrib.layers.variance_scaling_initializer()
W1 = tf.Variable(initializer([784,256]))
```
### Bias 초기화
bias의 경우 일반적으로 0으로 초기화하는 것이 효율적임

## Learning Rate Scheduler
```python
tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)

## 학습
callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                  epochs=15, callbacks=[callback], verbose=0)
```
## Data Normalization

## Various Optimizer

## Batch Normaliztion(Drop out)


### 하이퍼파라미터 튜닝 (K-Fold 모델 학습)
```python
### 케라스에서 K-Fold ###
from sklearn.model_selection import KFold

kfold = KFold(n_splits=5, shuffle = True) #n_splits 만큼 나눈다
for train, validation in kfold.split(x_data, y_data): #n_splits 만큼 반복
    hist = model.fit(datagen.flow(x_data, y_data, batch_size =32),                     
                    epochs = 60, 
                     steps_per_epoch = x_data.shape[0]//32, 
                     validation_data = (x_data[validation], y_data[validation]), 
                     callbacks = callbacks,
                     verbose = 1)
```
